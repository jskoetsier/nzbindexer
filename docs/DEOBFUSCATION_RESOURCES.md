# Comprehensive Deobfuscation Resources & Implementation Guide

**Last Updated:** 2025-11-20
**Purpose:** External resources, APIs, libraries, and implementation details for improving Usenet deobfuscation

---

## ðŸ“š TABLE OF CONTENTS

1. [Public ORN APIs & Databases](#public-orn-apis--databases)
2. [Archive Header Libraries](#archive-header-libraries)
3. [Open Source Indexer Projects](#open-source-indexer-projects)
4. [PreDB Services](#predb-services)
5. [Community Resources](#community-resources)
6. [Implementation Enhancements](#implementation-enhancements)
7. [Alternative Deobfuscation Methods](#alternative-deobfuscation-methods)

---

## ðŸ”Œ PUBLIC ORN APIs & DATABASES

### 1. NZBHydra2 (Already Implemented âœ“)
- **GitHub:** https://github.com/theotherp/nzbhydra2
- **Purpose:** Meta-indexer with aggregated hash database
- **Your Implementation:** `/app/services/nzbhydra.py`
- **API Endpoint:** `http://localhost:5076/api?apikey=XXX&t=search&q=HASH`
- **Database Size:** Millions of mappings from 50+ indexers
- **Cost:** Free (self-hosted)

**Setup Command:**
```bash
docker run -d \
  --name=nzbhydra2 \
  -p 5076:5076 \
  -v /path/to/config:/config \
  --restart unless-stopped \
  linuxserver/nzbhydra2
```

### 2. Newznab Protocol Indexers (Already Implemented âœ“)
- **Your Implementation:** `/app/services/newznab.py`
- **API Format:** `?t=search&apikey=XXX&q=query`

**Free/Freemium Indexers:**
- **NZBGeek:** https://nzbgeek.info
  - Lifetime: $15-30
  - API: `https://api.nzbgeek.info/api?apikey=XXX`
  - Rate Limit: 100/day (free), unlimited (paid)

- **DrunkenSlug:** https://drunkenslug.com
  - Free tier: 5 API calls/day
  - Premium: $20/year
  - API: `https://api.drunkenslug.com/api?apikey=XXX`

- **NZBFinder:** https://nzbfinder.ws
  - Free tier available
  - API: `https://nzbfinder.ws/api?apikey=XXX`

- **NinjaCentral:** https://www.nzbplanet.net
  - Invite-based
  - High-quality deobfuscation database

### 3. Prowlarr API
- **GitHub:** https://github.com/Prowlarr/Prowlarr
- **Purpose:** Indexer manager with hash resolution
- **API:** `http://localhost:9696/api/v1/search?query=HASH`
- **Advantage:** Aggregates 50+ indexers including private ones
- **Database Export:** Can export hash mappings periodically

**Setup:**
```bash
docker run -d \
  --name=prowlarr \
  -p 9696:9696 \
  -v /path/to/config:/config \
  --restart unless-stopped \
  linuxserver/prowlarr
```

---

## ðŸ“¦ ARCHIVE HEADER LIBRARIES

### Current Status
Your implementation (`/app/services/deobfuscation.py`) uses manual binary parsing. These libraries are more robust:

### 1. rarfile (Recommended for RAR)
```python
# Install: pip install rarfile
import rarfile
import io

def extract_rar_filename_library(data: bytes) -> Optional[str]:
    """Extract filename using rarfile library"""
    try:
        with rarfile.RarFile(io.BytesIO(data)) as rf:
            filenames = rf.namelist()
            if filenames:
                # Return first non-directory entry
                for name in filenames:
                    if not name.endswith('/'):
                        return name
        return None
    except rarfile.NotRarFile:
        return None
    except Exception as e:
        logger.debug(f"rarfile extraction failed: {e}")
        return None
```

**Advantages:**
- Handles RAR4 and RAR5 formats
- Proper CRC validation
- Unicode filename support
- Encrypted archive detection

**Installation:**
```bash
pip install rarfile
# Also need unrar binary:
# macOS: brew install unrar
# Ubuntu: apt-get install unrar
# Or use unar as backend
```

### 2. py7zr (Recommended for 7-Zip)
```python
# Install: pip install py7zr
import py7zr
import io

def extract_7z_filename_library(data: bytes) -> Optional[str]:
    """Extract filename using py7zr library"""
    try:
        with py7zr.SevenZipFile(io.BytesIO(data), mode='r') as archive:
            filenames = archive.getnames()
            if filenames:
                return filenames[0]
        return None
    except Exception as e:
        logger.debug(f"py7zr extraction failed: {e}")
        return None
```

**Advantages:**
- Pure Python implementation
- LZMA/LZMA2 compression support
- AES encryption detection
- Better than manual parsing for RAR5

**Installation:**
```bash
pip install py7zr
```

### 3. zipfile (Built-in, enhance current usage)
Your current ZIP implementation is good. Enhancement suggestion:

```python
import zipfile
import io

def extract_zip_filename_enhanced(data: bytes) -> List[str]:
    """Extract all filenames from ZIP (not just first)"""
    try:
        with zipfile.ZipFile(io.BytesIO(data)) as zf:
            # Get all filenames, filter directories
            filenames = [name for name in zf.namelist()
                        if not name.endswith('/')]
            return filenames
    except zipfile.BadZipFile:
        return []
```

### 4. pypar2 (For Par2 Files)
```python
# Install: pip install pypar2
from pypar2 import Par2File

def extract_par2_filenames_library(data: bytes) -> List[str]:
    """Extract filenames using pypar2 library"""
    try:
        par2 = Par2File(io.BytesIO(data))
        return par2.get_filenames()
    except Exception as e:
        logger.debug(f"pypar2 extraction failed: {e}")
        return []
```

**Note:** `pypar2` is not actively maintained. Your manual implementation may be better.

### 5. libarchive (Universal Archive Library)
```python
# Install: pip install libarchive-c
import libarchive

def extract_any_archive(data: bytes) -> Optional[str]:
    """Extract from any supported archive format"""
    try:
        with libarchive.memory_reader(data) as archive:
            for entry in archive:
                if not entry.isdir:
                    return entry.pathname
        return None
    except Exception as e:
        logger.debug(f"libarchive extraction failed: {e}")
        return None
```

**Supports:** RAR, 7z, ZIP, TAR, ISO, CAB, ARJ, LZH, and more

**Installation:**
```bash
pip install libarchive-c
# Requires system libarchive:
# macOS: brew install libarchive
# Ubuntu: apt-get install libarchive-dev
```

---

## ðŸ’» OPEN SOURCE INDEXER PROJECTS

### 1. NNTmux (Most Active)
- **GitHub:** https://github.com/NNTmux/newznab-tmux
- **Language:** PHP
- **Status:** Active development
- **Deobfuscation Code:** `nntmux/processing/ProcessReleases.php`

**Key Files to Study:**
```bash
git clone https://github.com/NNTmux/newznab-tmux
cd newznab-tmux

# Deobfuscation logic:
cat nntmux/processing/ProcessReleases.php | grep -A 50 "deobfuscate"

# Regex patterns for release detection:
cat nntmux/ReleaseRegex.php

# Database schema:
cat resources/db/schema/releases.sql
```

**Regex Pattern Examples from NNTmux:**
```php
// They use regex collections for matching
$regexes = [
    '/^(?P<name>.+?)\.S\d{1,3}E\d{1,3}/', // TV shows
    '/^(?P<name>.+?)\.\d{4}\./', // Movies with year
    '/^(?P<name>[A-Za-z0-9._-]+)\.REPACK/', // Repack releases
];
```

### 2. nZEDb (Archived but Valuable)
- **GitHub:** https://github.com/nZEDb/nZEDb
- **Language:** PHP
- **Status:** Archived (2018) but code still useful
- **ORN Database:** Had extensive hash mapping tables

**Download Database Schemas:**
```bash
git clone https://github.com/nZEDb/nZEDb
cd nZEDb

# Check release processing:
cat nZEDb/processing/post/ProcessReleases.php

# Hash lookup methods:
cat nZEDb/db/Release.php | grep -A 20 "searchHash"
```

### 3. Newznab (Original)
- **GitHub:** https://github.com/niel/newznab
- **Language:** PHP
- **Status:** Unmaintained (last update 2014)
- **Value:** Original Newznab protocol implementation

### 4. Spotweb (Dutch Alternative)
- **GitHub:** https://github.com/spotweb/spotweb
- **Language:** PHP
- **Focus:** Spotnet protocol (different from Usenet binary indexing)

### 5. PyNZB (Python Project - Abandoned)
- **GitHub:** https://github.com/ericraio/pynzb
- **Language:** Python
- **Status:** Abandoned
- **Value:** Python NZB parsing examples

---

## ðŸ—„ï¸ PREDB SERVICES

### Already Implemented (âœ“)
Your `/app/services/predb.py` uses these APIs:

1. **predb.ovh** - https://predb.ovh/api/v1/
2. **predb.me** - https://predb.me/api/v1/
3. **srrdb.com** - https://www.srrdb.com/api/
4. **abgx360.net** - https://abgx360.net/api/

### Additional PreDB Services

#### 1. predb.pw (Fast & Reliable)
```python
# Add to predb_apis in predb.py:
{
    "name": "predb.pw",
    "url": "https://predb.pw/api/v1/",
    "method": "search",
    "query_param": "q",
}
```

**API Example:**
```bash
curl "https://predb.pw/api/v1/search?q=Some.Release.Name"
```

#### 2. predb.de (German Scene DB)
```python
{
    "name": "predb.de",
    "url": "https://api.predb.de/v1/",
    "method": "search",
    "query_param": "q",
}
```

#### 3. orlydb.com (Scene Release Tracker)
- URL: https://orlydb.com
- API: Limited public access
- Focus: Scene releases with timestamps

#### 4. xREL.to (Movies & TV)
```python
{
    "name": "xrel.to",
    "url": "https://api.xrel.to/v2/",
    "method": "release/search.json",
    "query_param": "q",
    "requires_auth": True,  # Need API key
}
```

**API Docs:** https://www.xrel.to/wiki/6435/api-release-info.html

#### 5. Layer13 PreDB
- Invite-only
- High quality scene releases
- Historical data back to 1990s

---

## ðŸ‘¥ COMMUNITY RESOURCES

### Reddit Communities
1. **r/usenet** - https://reddit.com/r/usenet
   - Weekly discussion threads
   - Ask for ORN database dumps
   - Indexer operator discussions

2. **r/DataHoarder** - https://reddit.com/r/DataHoarder
   - Archive/preservation focus
   - Members share databases

3. **r/selfhosted** - https://reddit.com/r/selfhosted
   - DIY indexer operators
   - Technical discussions

### Discord Servers
Search for "Usenet Discord" - several active communities exist (invites change frequently)

### GitHub Topics to Search
```
topic:usenet topic:indexer
topic:nzb topic:deobfuscation
topic:predb
```

### GitHub Code Search Queries
```
"obfuscated release name" language:python
"usenet hash" "database" language:sql
"newznab" "deobfuscate"
"orn mapping" filetype:json
```

### Usenet Forums
- **UsenetInvites.com** - Community forums
- **NZBMatrix refugees** - Various forums (NZBMatrix shut down 2012, communities scattered)

---

## ðŸ”§ IMPLEMENTATION ENHANCEMENTS

### 1. Enhanced yEnc Decoder

**Issue:** Your current decoder is good but could be optimized.

**Enhancement:**
```python
# Add to YEncDecoder class
@staticmethod
def decode_optimized(body_lines: List[str], max_bytes: int = 10240) -> Optional[bytes]:
    """Optimized yEnc decoder using bytearray operations"""
    try:
        decoded = bytearray()
        in_data = False

        for line in body_lines:
            if isinstance(line, bytes):
                line = line.decode('latin-1', errors='ignore')

            if line.startswith('=ybegin'):
                in_data = True
                continue
            elif line.startswith('=yend'):
                break
            elif line.startswith('=ypart'):
                continue

            if in_data and line and not line.startswith('='):
                # Process entire line at once for speed
                line = line.rstrip('\r\n')
                line_bytes = line.encode('latin-1')

                i = 0
                while i < len(line_bytes):
                    if line_bytes[i] == ord('='):
                        # Next byte is escaped
                        if i + 1 < len(line_bytes):
                            decoded.append((line_bytes[i+1] - 64) % 256)
                            i += 2
                        else:
                            i += 1
                    else:
                        decoded.append((line_bytes[i] - 42) % 256)
                        i += 1

                    if len(decoded) >= max_bytes:
                        return bytes(decoded)

        return bytes(decoded) if decoded else None

    except Exception as e:
        logger.debug(f"Error in optimized yEnc decode: {e}")
        return None
```

### 2. NFO File Parser

**Add to deobfuscation.py:**

```python
class NFOParser:
    """Parser for NFO (release info) files"""

    @staticmethod
    def extract_release_name(data: bytes) -> Optional[str]:
        """
        Extract release name from NFO file

        NFO files are plain text with release information.
        Common formats:
        - Release: Some.Release.Name
        - Title: Some.Release.Name
        - [Some.Release.Name]
        """
        try:
            # NFO files are typically ASCII or CP437 (DOS encoding)
            try:
                text = data.decode('utf-8')
            except UnicodeDecodeError:
                text = data.decode('cp437', errors='ignore')

            # Common NFO patterns
            patterns = [
                r'Release[:\s]+([A-Za-z0-9._-]+)',
                r'Title[:\s]+([A-Za-z0-9._-]+)',
                r'\[([A-Za-z0-9._-]+\.\d{4}\.[A-Za-z0-9._-]+)\]',
                r'Name[:\s]+([A-Za-z0-9._-]+)',
            ]

            for pattern in patterns:
                match = re.search(pattern, text, re.IGNORECASE)
                if match:
                    release_name = match.group(1).strip()
                    # Validate it looks like a release name
                    if (len(release_name) > 10 and
                        '.' in release_name and
                        not release_name.startswith('http')):
                        logger.info(f"Extracted release from NFO: {release_name}")
                        return release_name

            return None

        except Exception as e:
            logger.debug(f"Error parsing NFO: {e}")
            return None
```

### 3. SFV File Parser

**Add to deobfuscation.py:**

```python
class SFVParser:
    """Parser for SFV (Simple File Verification) files"""

    @staticmethod
    def extract_filenames(data: bytes) -> List[str]:
        """
        Extract filenames from SFV file

        SFV format:
        filename.ext DEADBEEF
        """
        filenames = []
        try:
            text = data.decode('utf-8', errors='ignore')

            for line in text.split('\n'):
                line = line.strip()
                # SFV format: filename CRC32
                if line and not line.startswith(';'):  # ; is comment
                    parts = line.split()
                    if len(parts) >= 2:
                        # First part is filename, last part is CRC
                        filename = ' '.join(parts[:-1])
                        if '.' in filename and len(filename) > 5:
                            filenames.append(filename)

            if filenames:
                logger.info(f"Extracted {len(filenames)} filenames from SFV")

            return filenames

        except Exception as e:
            logger.debug(f"Error parsing SFV: {e}")
            return []
```

### 4. Multi-Threading for Archive Parsing

**Optimize deobfuscation.py:**

```python
import concurrent.futures

class DeobfuscationService:
    """Enhanced with parallel processing"""

    def extract_filename_parallel(
        self, body_lines: List[str], yenc_filename: str
    ) -> Optional[str]:
        """
        Try all archive formats in parallel for faster processing
        """
        decoded_data = self.yenc_decoder.decode(body_lines, max_bytes=10240)
        if not decoded_data:
            return None

        # Try all formats in parallel
        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
            futures = {
                executor.submit(self.rar_parser.extract_filename, decoded_data): 'RAR',
                executor.submit(self.zip_parser.extract_filename, decoded_data): 'ZIP',
                executor.submit(self.sevenzip_parser.extract_filename, decoded_data): '7Z',
                executor.submit(self.par2_parser.extract_filenames, decoded_data): 'PAR2',
            }

            for future in concurrent.futures.as_completed(futures):
                result = future.result()
                if result:
                    if isinstance(result, list):
                        # Par2 returns list
                        for fn in result:
                            if not fn.endswith('.par2'):
                                return fn
                    else:
                        return result

        return None
```

### 5. Hash Detection Enhancements

**Add to DeobfuscationService.is_obfuscated_hash():**

```python
def is_obfuscated_hash_enhanced(self, filename: str) -> bool:
    """Enhanced hash detection with more patterns"""
    name_no_ext = filename

    # Strip extensions
    while True:
        before = name_no_ext
        name_no_ext = re.sub(
            r'\.(rar|par2?|zip|7z|nfo|sfv|r\d{2,3}|part\d+|vol\d+\+?\d*)$',
            '', name_no_ext, flags=re.IGNORECASE
        )
        if name_no_ext == before:
            break

    name_no_ext = name_no_ext.strip('.-_')

    # Extended patterns
    patterns = [
        r'^[a-fA-F0-9]{32}$',           # MD5
        r'^[a-fA-F0-9]{40}$',           # SHA1
        r'^[a-fA-F0-9]{64}$',           # SHA256
        r'^[a-fA-F0-9]{16,}$',          # Generic hex
        r'^[a-zA-Z0-9_-]{22,}$',        # Base64-like
        r'^[a-zA-Z0-9]{18,}$',          # Pure alphanumeric
        r'^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$',  # UUID
        r'^[A-Z0-9]{10,}$',             # All caps random
        r'^\d{10,}$',                   # Unix timestamp or numeric
    ]

    for pattern in patterns:
        if re.match(pattern, name_no_ext):
            return True

    # Check for lack of meaningful words
    if len(name_no_ext) < 10 and not re.search(r'[a-z]{3,}', name_no_ext.lower()):
        return True

    return False
```

---

## ðŸš€ ALTERNATIVE DEOBFUSCATION METHODS

### 1. File Size Fingerprinting

```python
class SizeBasedMatcher:
    """Match releases by file size patterns"""

    async def match_by_size(
        self,
        size_bytes: int,
        post_date: datetime,
        category: str
    ) -> Optional[str]:
        """
        Match obfuscated release by size + metadata

        Uses TMDB/IMDB data to find releases with similar size
        released around the same date.
        """
        # Query TMDB for releases within Â±7 days
        date_range = (post_date - timedelta(days=7), post_date + timedelta(days=7))

        # Get candidates from TMDB (you already have this service)
        candidates = await self.tmdb_service.get_recent_releases(
            date_range=date_range,
            category=category
        )

        # Match by size (within Â±5%)
        tolerance = 0.05
        matches = []

        for candidate in candidates:
            if candidate.size:
                size_diff = abs(candidate.size - size_bytes) / size_bytes
                if size_diff < tolerance:
                    matches.append({
                        'name': candidate.name,
                        'confidence': 1.0 - size_diff,
                        'size_match': candidate.size,
                    })

        # Return best match
        if matches:
            best = max(matches, key=lambda x: x['confidence'])
            if best['confidence'] > 0.90:
                return best['name']

        return None
```

### 2. Cross-Reference with Download Clients

```python
class SABnzbdMiner:
    """Mine hash mappings from SABnzbd history"""

    def extract_from_sabnzbd_db(self, db_path: str) -> List[Dict]:
        """
        Extract obfuscated -> real name mappings from SABnzbd

        SABnzbd stores download history with both the obfuscated
        NZB name and the extracted real name.
        """
        import sqlite3

        mappings = []

        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()

            query = """
            SELECT
                nzb_name as obfuscated,
                report_name as real_name,
                bytes as size
            FROM history
            WHERE nzb_name != report_name
              AND nzb_name LIKE '%[A-Fa-f0-9]%'
            """

            cursor.execute(query)

            for row in cursor.fetchall():
                mappings.append({
                    'obfuscated_hash': row[0],
                    'real_name': row[1],
                    'size': row[2],
                    'source': 'sabnzbd_history',
                    'confidence': 0.95
                })

            conn.close()

            logger.info(f"Extracted {len(mappings)} mappings from SABnzbd")
            return mappings

        except Exception as e:
            logger.error(f"Error mining SABnzbd database: {e}")
            return []
```

### 3. Community Collaborative Database

**You already have the API endpoints (`/app/api/v1/endpoints/orn.py`)**

**Enhancement - Add automatic sync:**

```python
class ORNSyncService:
    """Automatically sync with partner indexers"""

    def __init__(self):
        # List of partner indexers to sync with
        self.partners = [
            'https://partner1.com/api/v1/orn/public/mappings',
            'https://partner2.com/api/v1/orn/public/mappings',
        ]

    async def sync_from_partners(self, limit: int = 1000) -> int:
        """
        Sync hash mappings from partner indexers

        Returns: Number of new mappings imported
        """
        total_imported = 0

        async with aiohttp.ClientSession() as session:
            for partner_url in self.partners:
                try:
                    async with session.get(
                        partner_url,
                        params={'limit': limit}
                    ) as response:
                        if response.status == 200:
                            data = await response.json()

                            # Import to local database
                            for mapping in data.get('mappings', []):
                                # Only import high-confidence mappings
                                if mapping.get('confidence', 0) >= 0.8:
                                    await self.import_mapping(mapping)
                                    total_imported += 1

                except Exception as e:
                    logger.error(f"Error syncing from {partner_url}: {e}")

        logger.info(f"Synced {total_imported} new mappings from partners")
        return total_imported
```

---

## ðŸ“Š PERFORMANCE METRICS

### Recommended Tracking

Add these metrics to monitor deobfuscation effectiveness:

```python
class DeobfuscationMetrics:
    """Track deobfuscation performance"""

    async def get_success_rates(self) -> Dict:
        """Get success rates by method"""

        return {
            'total_attempts': 10000,
            'success_rate': 0.67,
            'by_method': {
                'archive_header': {'attempts': 3000, 'success': 2400, 'rate': 0.80},
                'predb_lookup': {'attempts': 5000, 'success': 3500, 'rate': 0.70},
                'nzbhydra2': {'attempts': 4000, 'success': 2800, 'rate': 0.70},
                'hash_decode': {'attempts': 2000, 'success': 200, 'rate': 0.10},
                'newznab': {'attempts': 1000, 'success': 600, 'rate': 0.60},
            },
            'cache_hit_rate': 0.45,
            'avg_lookup_time_ms': 250,
        }
```

---

## âœ… IMMEDIATE ACTION PLAN

### Week 1: Library Upgrades
```bash
# Install better archive libraries
pip install rarfile py7zr libarchive-c

# Update deobfuscation.py to use libraries instead of manual parsing
```

### Week 2: API Expansion
- Add predb.pw to PreDB service
- Set up Prowlarr integration
- Get API keys from NZBGeek (free tier)

### Week 3: Community Integration
- Post on r/usenet about your public ORN API
- Set up automatic partner syncing
- Search GitHub for community databases

### Week 4: Advanced Features
- Implement NFO/SFV parsing
- Add size-based fingerprinting
- Set up SABnzbd history mining (if applicable)

---

## ðŸ“š FURTHER READING

### Academic Papers
- "Content-Based File Type Detection" (file signature research)
- "Usenet Archive Forensics" (various security papers)

### Technical Documentation
- RFC 3977: Network News Transfer Protocol (NNTP)
- yEnc specification: http://www.yenc.org
- RAR format: https://www.rarlab.com/technote.htm
- Par2 specification: https://parchive.github.io

### Blogs & Articles
- Search "usenet obfuscation" on Medium
- DeviantSafe blog (Usenet security)
- NZB360 developer blog

---

## ðŸ”— QUICK REFERENCE LINKS

**Your Current Implementation:**
- Deobfuscation: `/app/services/deobfuscation.py`
- PreDB: `/app/services/predb.py`
- NZBHydra2: `/app/services/nzbhydra.py`
- Newznab: `/app/services/newznab.py`
- ORN API: `/app/api/v1/endpoints/orn.py`
- Guide: `/docs/ORN_DATABASE_GUIDE.md`

**GitHub Topics:**
```
https://github.com/topics/usenet
https://github.com/topics/nzb
https://github.com/topics/indexer
https://github.com/topics/newznab
```

**Communities:**
- r/usenet: https://reddit.com/r/usenet
- r/DataHoarder: https://reddit.com/r/datahoarder
- r/selfhosted: https://reddit.com/r/selfhosted

---

**Last Updated:** 2025-11-20
**Next Review:** Add new APIs/libraries as they become available
